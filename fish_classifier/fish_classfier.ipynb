{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fish classifier\n",
    "\n",
    "Author: Łukasz Szarecki \n",
    "\n",
    "Dataset: https://www.kaggle.com/aungpyaeap/fish-market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Uploading data to pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Species  Weight  Length1  Length2  Length3   Height   Width\n",
      "0   Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
      "1   Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
      "2   Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
      "3   Bream   363.0     26.3     29.0     33.5  12.7300  4.4555\n",
      "4   Bream   430.0     26.5     29.0     34.0  12.4440  5.1340\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "fish_df = pd.read_csv(\"Fish.csv\")\n",
    "print(fish_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We have to mix data because there are sorted.\n",
    "3. Preapering training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Species  Weight  Length1  Length2  Length3   Height   Width\n",
      "0   Perch  1100.0     40.1     43.0     45.5  12.5125  7.4165\n",
      "1   Perch   250.0     25.9     28.0     29.4   7.8204  4.2042\n",
      "2   Smelt    10.0     11.3     11.8     13.1   2.2139  1.2838\n",
      "3   Perch    51.5     15.0     16.2     17.2   4.5924  2.6316\n",
      "4   Bream   725.0     31.8     35.0     40.9  16.3600  6.0532\n",
      "There are 159 samples\n"
     ]
    }
   ],
   "source": [
    "#mixing dataset\n",
    "fish_df = fish_df.sample(frac=1)\n",
    "#corect indexing\n",
    "fish_df = fish_df.reset_index(drop=True)\n",
    "print(fish_df.head())\n",
    "print(f'There are {len(fish_df)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 6)\n",
      "(159,)\n"
     ]
    }
   ],
   "source": [
    "#input\n",
    "X = np.array(fish_df.iloc[:, 1::])\n",
    "print(X.shape)\n",
    "#output\n",
    "Y = (fish_df.iloc[:,0])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder class\n",
    "\n",
    "3. Each fish will have its own identifier\n",
    "* fit() - create connection between identifiers and fish names\n",
    "* transform() - fish name (str) to id (int)\n",
    "* inverse_transform() - id (int) to fish name (str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self):\n",
    "        self.names = [] #to not store duplicated names\n",
    "        self.num_classes = 0 #number of all classes\n",
    "    def fit(self, y):\n",
    "        for sample in y:\n",
    "            if sample not in self.names:\n",
    "                self.names.append(sample)\n",
    "        self.num_classes = len(self.names)\n",
    "    def transform(self, y):\n",
    "        encoded_samples = np.zeros(len(y))\n",
    "        for index,sample in enumerate(y):\n",
    "            encoded_samples[index] = self.names.index(sample)\n",
    "        return encoded_samples\n",
    "    def inverse_transform(self, encoded_y):\n",
    "        samples = []\n",
    "        for sample in encoded_y:\n",
    "            samples.append(self.names[int(sample)])\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Perch', 'Smelt', 'Bream', 'Parkki', 'Roach', 'Pike', 'Whitefish']\n",
      "7\n",
      "['Pike', 'Smelt', 'Perch', 'Pike', 'Pike', 'Bream', 'Bream', 'Roach']\n",
      "['Pike', 'Smelt', 'Perch', 'Pike', 'Pike', 'Bream', 'Bream', 'Roach']\n"
     ]
    }
   ],
   "source": [
    "#tests\n",
    "encoder = Encoder()\n",
    "encoder.fit(Y)\n",
    "print(encoder.names)\n",
    "print(encoder.num_classes) \n",
    "\n",
    "test_list = ['Pike', 'Smelt', 'Perch', 'Pike', 'Pike', 'Bream', 'Bream', 'Roach']\n",
    "encoded_num_test = encoder.transform(test_list)\n",
    "print(test_list)\n",
    "print(encoder.inverse_transform(encoded_num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Encoding dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "encoder.fit(Y)\n",
    "encoded_y = encoder.transform(Y)\n",
    "# print(encoded_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling - Normalization\n",
    "\n",
    "* test fraction - percentage of test data\n",
    "* xte - test data\n",
    "* xtr - training data\n",
    "\n",
    "xtr ->\n",
    "* mean = 0\n",
    "* standard deviation = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, x,y,test_fraction=0.05):\n",
    "        test_samples = int(test_fraction * y.size)\n",
    "        self.xte = x[:test_samples,:]   \n",
    "        self.xtr = x[test_samples:,:]   \n",
    "        self.yte = y[:test_samples]   \n",
    "        self.ytr = y[test_samples:]\n",
    "        self.mean = np.mean(self.xtr, axis=0)\n",
    "        self.std = np.std(self.xtr, axis=0)#standard deviation\n",
    "        self.xtr = self.normalize(self.xtr)\n",
    "        self.xte = self.normalize(self.xte)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        x_temp = (x - self.mean)/self.std\n",
    "        return x_temp\n",
    "\n",
    "# creating dataset        \n",
    "fish_ds = Dataset(X,encoded_y,0.20)\n",
    "# fish_ds.xte \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers models\n",
    "\n",
    "- Random Classifier\n",
    "- KNN - K - Nearest Neighbors Algorithm\n",
    "- Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classfier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, xtr, ytr):\n",
    "        pass\n",
    "    def predict(self, x):\n",
    "        pass\n",
    "    def evaluate(self, xte, yte):\n",
    "        ypred = self.predict(xte)\n",
    "        print(yte.astype(int))\n",
    "        print(ypred)\n",
    "        acc = np.sum(ypred == yte) / yte.size #Accuracy\n",
    "        return acc\n",
    "\n",
    "# Baseline (method)\n",
    "class RandomClassifier(Classfier):\n",
    "    #expected accuracy = 1/7\n",
    "    def fit(self, xtr, ytr):\n",
    "        self.num_classes = int(np.max(ytr) + 1)\n",
    "    def predict(self, x):\n",
    "        return np.random.choice(self.num_classes, x.shape[0])\n",
    "\n",
    "class KNNClassifier(Classfier):\n",
    "    def __init__(self, k=3):   #hyperparameters\n",
    "        self.k = k\n",
    "    def fit(self, xtr, ytr):\n",
    "        self.xtr = xtr\n",
    "        self.ytr = ytr\n",
    "    def predict(self, x):\n",
    "        num_samples =  x.shape[0]\n",
    "        ypred = np.zeros(num_samples, dtype=int)\n",
    "        for i in range(num_samples):\n",
    "            distance = np.sum((self.xtr - x[i, :])**2, axis=1)\n",
    "            order = np.argsort(distance)\n",
    "            knn_label = self.ytr[order][:self.k].astype(int)\n",
    "            # print(f'knn labels {knn_label}')\n",
    "            binc = np.bincount(knn_label)\n",
    "            # print(f'Bin count {binc}')\n",
    "            # print(f'Y pred {binc.argmax()}')\n",
    "            ypred[i] = binc.argmax()    #argument which store max value\n",
    "        return ypred    \n",
    "        \n",
    "class LNRClassifier(Classfier):\n",
    "    def __init__ (self, num_inputs, num_classes, lr = 0.001):\n",
    "        self.W = np.random.randn(num_inputs, num_classes)/num_inputs\n",
    "        self.b = np.zeros(num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "    def fit(self, xtr, ytr, num_epochs=1):\n",
    "        for e in range(num_epochs):\n",
    "            z = np.dot(xtr, self.W) + self.b\n",
    "            ypred = self._softmax(z)\n",
    "            loss = self._cross_entropy_loss(ypred, ytr)\n",
    "            if e%100 == 99:\n",
    "                print(f'Epoch: {e+1}, loss: {loss:.3}, acc: {self._acc(ytr,ypred):.3}')\n",
    "            dz = ypred - self._one_hot(ytr)\n",
    "            dW = np.dot(xtr.T, dz)\n",
    "            db = np.sum(dz, axis=0)\n",
    "            self.W -= dW * self.lr\n",
    "            self.b -= db * self.lr\n",
    "    def _softmax(self, z):\n",
    "        max_z = np.max(z, axis=1, keepdims=True)\n",
    "        return np.exp(z - max_z) / np.sum(np.exp(z - max_z), axis=1, keepdims=True)\n",
    "    def _cross_entropy_loss(self, ypred, ytr):\n",
    "        ytr_one_hot = self._one_hot(ytr)\n",
    "        return np.mean(-ytr_one_hot * np.log(ypred))\n",
    "    def _one_hot(self, ytr):\n",
    "        ytr_one_hot = np.zeros((ytr.size, self.num_classes))\n",
    "        ytr_one_hot[np.arange(ytr.size), ytr.astype(int)] = 1\n",
    "        return ytr_one_hot\n",
    "    def _acc(self, ytr, ypred):\n",
    "        ypred = np.argmax(ypred, axis=1)\n",
    "        return np.mean(ytr == ypred)\n",
    "    def predict(self, x):\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        ypred = self._softmax(z)\n",
    "        ypred = np.argmax(ypred, axis=1)\n",
    "        return ypred\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 2 2 2 3 2 2 2 0 0 4 2 0 0 1 0 3 4 4 1 5 2 0 1 0 0 3 0]\n",
      "[6 1 4 6 4 5 6 2 6 2 2 5 2 5 4 0 5 1 4 6 0 2 6 0 3 0 5 2 3 3 4]\n",
      "Random classifier accuracy\n",
      " 0.1935483870967742\n",
      "[0 0 1 0 2 2 2 3 2 2 2 0 0 4 2 0 0 1 0 3 4 4 1 5 2 0 1 0 0 3 0]\n",
      "[0 0 1 0 2 2 2 3 2 2 2 0 0 0 2 0 0 1 0 3 0 0 1 5 2 0 1 0 0 3 1]\n",
      "KNN classifier accuracy\n",
      " 0.8709677419354839\n",
      "Epoch: 100, loss: 0.132, acc: 0.742\n",
      "Epoch: 200, loss: 0.11, acc: 0.742\n",
      "Epoch: 300, loss: 0.0991, acc: 0.766\n",
      "Epoch: 400, loss: 0.0918, acc: 0.797\n",
      "Epoch: 500, loss: 0.0865, acc: 0.805\n",
      "Epoch: 600, loss: 0.0824, acc: 0.805\n",
      "Epoch: 700, loss: 0.079, acc: 0.805\n",
      "Epoch: 800, loss: 0.0763, acc: 0.805\n",
      "Epoch: 900, loss: 0.0739, acc: 0.805\n",
      "Epoch: 1000, loss: 0.0718, acc: 0.805\n",
      "[0 0 1 0 2 2 2 3 2 2 2 0 0 4 2 0 0 1 0 3 4 4 1 5 2 0 1 0 0 3 0]\n",
      "[0 0 1 0 2 2 2 3 2 2 2 0 0 0 2 0 0 1 0 3 0 4 1 5 2 0 1 0 0 3 1]\n",
      "lnr classifier accuracy\n",
      " 0.9032258064516129\n"
     ]
    }
   ],
   "source": [
    "random_classifier = RandomClassifier()\n",
    "random_classifier.fit(fish_ds.xtr, fish_ds.ytr)\n",
    "randow_acc = random_classifier.evaluate(fish_ds.xte, fish_ds.yte)\n",
    "print(f'Random classifier accuracy\\n {randow_acc}')\n",
    "\n",
    "knn_classifier = KNNClassifier(4)\n",
    "knn_classifier.fit(fish_ds.xtr, fish_ds.ytr)\n",
    "knn_acc = knn_classifier.evaluate(fish_ds.xte, fish_ds.yte)\n",
    "print(f'KNN classifier accuracy\\n {knn_acc}')\n",
    "\n",
    "lnr_classifier = LNRClassifier(X.shape[1], int(np.max(encoded_y) + 1))\n",
    "lnr_classifier.fit(fish_ds.xtr, fish_ds.ytr, 1000)\n",
    "lnr_acc = lnr_classifier.evaluate(fish_ds.xte, fish_ds.yte)\n",
    "print(f'lnr classifier accuracy\\n {lnr_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pike']\n"
     ]
    }
   ],
   "source": [
    "#New fish test\n",
    "new_fish = np.array([[1000, 60, 60, 70, 15, 5]])\n",
    "new_fish_pred = knn_classifier.predict(new_fish)\n",
    "print(encoder.inverse_transform(new_fish_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note [PL]**\n",
    "Każdy model w uczeniu maszynowym ma parametry. Jedne mają ich więcej drugie mniej. Jeśli parametr wyliczany jest samodzielnie przez algorytm podczas uczenia nazywamy go po prostu parametrem. Przykładem mogą być wagi w sieciach neuronowych.\n",
    "\n",
    "Natomiast jeśli parametr podawany jest przez użytkownika, który używa algorytmu, wówczas nazywamy go hiperparametrem"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5d8d3d050c10ab3dcb6699cc248e53317db60ba2858504a06e019b80fca7572"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
